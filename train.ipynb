{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5311dfd",
   "metadata": {},
   "source": [
    "# ResNet18: Age-Gender Estimation Model\n",
    "\n",
    "## Why Age and Gender Prediction Matters\n",
    "Ever wondered how apps can guess your age or tailor content to you? That’s where age and gender prediction comes in! It’s used in:\n",
    "\n",
    "- **Personalization:** Ads and content that actually match your demographic.  \n",
    "- **Security:** Monitoring or authentication systems that need to know who’s who.  \n",
    "- **Healthcare:** Studying age-related trends or spotting conditions early.  \n",
    "- **Being bored:** If you’re someone like me who gets bored, why not dive into a fun project like this?\n",
    "\n",
    "By teaching AI to estimate age and gender from images, we can make smarter, more human-aware systems.\n",
    "\n",
    "## Dataset\n",
    "For this project, we are using the **UTKFace** dataset. It’s a massive collection of faces from ages 0 to 116, along with gender labels. The dataset also includes ethnicity and timestamps, but we’ll stick to age and gender for simplicity.\n",
    "\n",
    "## Image Preprocessing\n",
    "Before feeding images into our model, we do some basic prep:\n",
    "\n",
    "- **Resize:** Make sure all images are the same size.  \n",
    "- **Normalize:** Scale pixel values so the model learns faster and better.  \n",
    "- **Data Augmentation (optional):** Flip, rotate, or crop images to make the model more robust.\n",
    "\n",
    "These steps ensure our model sees clean, consistent images and can actually learn meaningful features from them.\n",
    "\n",
    "## Model\n",
    "We’re using **ResNet18**, a type of CNN (Convolutional Neural Network) introduced by Microsoft in 2015. The “18” refers to the number of layers with trainable weights.\n",
    "\n",
    "Explaining how Residual networks are helping us in **very deep networks** requires advanced mathematical knowledge . Here is a short and simple answer: \n",
    "Deep networks can run into **vanishing gradients**—gradients shrink as they move backward through layers, making learning slow or impossible. Residual connections in ResNet fix this, letting gradients **flow smoothly** and helping the network actually learn.\n",
    "Think of it like this: the network wants to pass information backward through many layers, but without shortcuts, it gets tired and loses signal. Residual connections give it a “fast lane” to keep the learning alive.\n",
    "\n",
    "## How It Works ?\n",
    "1. The model takes a facial image as input.  \n",
    "2. Convolutional layers extract features like eyes, nose, and mouth patterns.  \n",
    "3. Residual connections make sure these features are propagated effectively.  \n",
    "4. Fully connected layers finally predict **age** (as a number) and **gender** (as male/female).  \n",
    "\n",
    "This simple pipeline allows us to go from raw images to meaningful predictions, all in one go!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "049ea626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Imports ----------\n",
    "\n",
    "# Standard libraries\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import math\n",
    "from typing import Tuple\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms, models\n",
    "from torch.nn import L1Loss\n",
    "\n",
    "# Image handling\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# Kaggle dataset helper\n",
    "import kagglehub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559619b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Downloading UTKFace Dataset ----------\n",
    "\n",
    "# Download the dataset from Kaggle\n",
    "path = kagglehub.dataset_download(\"jangedoo/utkface-new\") # requires kagglehub.login() if you are not on Colab.\n",
    "\n",
    "# Copy the folder with cropped images to current directory\n",
    "# Note: using Python instead of shell for compatibility\n",
    "import shutil\n",
    "shutil.copytree(os.path.join(path, \"crop_part1\"), \"crop_part1\")\n",
    "\n",
    "# crop_part1 contains cropped face images for better age/gender prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc124f17",
   "metadata": {},
   "source": [
    "### Reproducibility & Device Setup\n",
    "- Setting **seeds** ensures that the results are consistent every time you run the notebook.  \n",
    "- Using **CUDA** if available, otherwise falling back to CPU, so training works on any machine.\n",
    "\n",
    "It's better you have CUDA if your grandchild is not the one who wants to see the end of training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cebcc081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ---------- Repro & Device ----------\n",
    "\n",
    "# Reproducibility\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Device selection: GPU if available, else CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52926b67",
   "metadata": {},
   "source": [
    "### Preparing the UTKFace Dataset \n",
    "\n",
    "UTKFace filenames look like this: `age_gender_race_date.jpg`  \n",
    "(e.g., `25_0_3_20170116174525125.jpg`). From this, we can sneakily grab **age** and **gender** for our model.\n",
    "\n",
    "Here’s what's going on:\n",
    "\n",
    "1. **Custom Dataset Class**\n",
    "    - `UTKFaceAgeGender` is our little helper that knows how to read images and labels.\n",
    "    - It grabs images from the folder, checks filenames for the right format, and extracts **age** and **gender**.\n",
    "    - When you ask for an item (`__getitem__`), it gives you:\n",
    "        - The processed image  \n",
    "        - `gender` as a float tensor (binary, 0 or 1)  \n",
    "        - `age` as a float tensor (number, regression style)\n",
    "\n",
    "2. **Transforms / Image Magic**\n",
    "    - Training images get a little workout:  \n",
    "        - Resize to `224x224` (ResNet loves this size)  \n",
    "        - Random flips and rotations (It's a widely used technique in computer vision for better accuracy)  \n",
    "        - Color tweaks (brightness & contrast)  \n",
    "        - Normalization so the network doesn’t freak out\n",
    "    - Validation images stay clean and neat—no tricks here.\n",
    "\n",
    "3. **Train/Validation Split**\n",
    "    - 80% training, 20% validation.  \n",
    "    - Thanks to our earlier **seed magic**, the split is always the same.  \n",
    "    - `DataLoader` batches images, shuffles training data, and uses multiple workers so our CPU/GPU doesn’t get bored.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "119476d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Extracting Images & Labels ----------\n",
    "\n",
    "_fname_re = re.compile(r'^(\\d+)_([01])_([0-4])_') \n",
    "\n",
    "\n",
    "class UTKFaceAgeGender(Dataset):\n",
    "    def __init__(self, folder_path: str, transform: transforms.Compose):\n",
    "        self.folder_path = folder_path\n",
    "        files = [f for f in os.listdir(folder_path) if f.lower().endswith('.jpg')] # Taking only jpg files \n",
    "        self.image_files = [f for f in files if _fname_re.match(f)] # Keep only correctly UTKFace formatted files\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files) # Just in case if you need count of images\n",
    "\n",
    "    def _parse_labels(self, fname: str) -> Tuple[float, float]:\n",
    "        # Extract age & gender\n",
    "        # Regex guarantees format; still guard:\n",
    "        parts = fname.split('_')\n",
    "        age = float(parts[0])\n",
    "        gender = float(parts[1])  # 0 or 1\n",
    "        return age, gender\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        fname = self.image_files[idx]\n",
    "        path = os.path.join(self.folder_path, fname)\n",
    "        img = Image.open(path).convert('RGB')\n",
    "\n",
    "        age, gender = self._parse_labels(fname)\n",
    "        img = self.transform(img)\n",
    "\n",
    "        # Return as shape (1,) float tensors\n",
    "        age_t = torch.tensor([age], dtype=torch.float32)\n",
    "        gender_t = torch.tensor([gender], dtype=torch.float32)   # for BCEWithLogits\n",
    "\n",
    "        return img, gender_t, age_t\n",
    "    \n",
    "# ---------- Transforms ----------\n",
    "\n",
    "train_tf = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "val_tf = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# --------- Creating train and validation data ----------\n",
    "\n",
    "data_dir = \"crop_part1\"  # <- set to your folder containing UTKFace images\n",
    "full_train = UTKFaceAgeGender(data_dir, transform=train_tf)\n",
    "n_total = len(full_train)\n",
    "n_val = int(0.2 * n_total)\n",
    "n_train = n_total - n_val\n",
    "train_ds, val_ds = random_split(full_train, [n_train, n_val])\n",
    "# ensure val has deterministic transforms\n",
    "val_ds.dataset.transform = val_tf\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=64, shuffle=False, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25b3f78",
   "metadata": {},
   "source": [
    "### Creating the Age & Gender Model\n",
    "\n",
    "So what’s `self.backbone` anyway?  \n",
    "\n",
    "- ResNet18 was originally trained on **ImageNet**, a huge dataset with **1000 classes** like cats, dogs, cars… you name it.  \n",
    "- But we don’t care about cats or dogs here—we only want **age** and **gender**.  \n",
    "- `self.backbone` is basically the ResNet18 network **without its final classification layer**, so we can repurpose it (call “personalize” it) for our task.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bd20517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------- Creating Model -------\n",
    "\n",
    "class AgeGenderNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Load pre-trained ResNet18 (ImageNet weights)\n",
    "        base = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "        \n",
    "        # Keep all layers except the final fully-connected layer which I just talked about .\n",
    "        self.backbone = nn.Sequential(*list(base.children())[:-1])\n",
    "        \n",
    "        # Add custom heads for our two tasks\n",
    "        self.gender_head = nn.Linear(512, 1)  # Binary classification\n",
    "        self.age_head = nn.Linear(512, 1)     # Regression\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten feature map\n",
    "        return self.gender_head(x), self.age_head(x)\n",
    "\n",
    "# -------- Defining Model & Moving It To GPU/CPU --------\n",
    "\n",
    "model = AgeGenderNet().to(device)\n",
    "\n",
    "# Freeze backbone so we don't retrain the whole ResNet\n",
    "for p in model.backbone.parameters():\n",
    "    p.requires_grad = False  # Only train the heads (This will be changed later , I'll explain down below)\n",
    "\n",
    "# -------- Loss Functions & Optimizer --------\n",
    "\n",
    "loss_gender = nn.BCEWithLogitsLoss()  # Binary classification for gender.\n",
    "loss_age = nn.SmoothL1Loss()          # Regression for age (SmoothL1 is less sensitive to outliers than MSE, but smoother than MAE)\n",
    "\n",
    "opt = optim.Adam([\n",
    "    {\"params\": model.gender_head.parameters()},\n",
    "    {\"params\": model.age_head.parameters()}\n",
    "], lr=1e-3)  # Train both heads using Adam optimizer (lr is learning-rate and 1e-3  if you don't know , it's 0.001 which is the default rate for Adam)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea50be6e",
   "metadata": {},
   "source": [
    "### Training the Model (No Harry Potter Here)\n",
    "\n",
    "So what’s happening in this loop? Don’t worry, no magic—just some good old **training tricks** to make sure our model doesn’t overfit or freak out.  \n",
    "\n",
    "1. **Patience & Early Stopping**\n",
    "    - We keep track of `total_loss` (age loss + gender loss) each epoch.  \n",
    "    - If the validation loss doesn’t improve for `patience` epochs, we stop training early.  \n",
    "    - This prevents wasting time and overfitting our model to the training set.\n",
    "\n",
    "2. **Freezing & Unfreezing the Backbone**\n",
    "    - At first, we **freeze the ResNet18 backbone** because our new heads (age & gender) aren’t trained yet.  \n",
    "    - Pretrained backbone features (from ImageNet) give a good starting point for learning.  \n",
    "    - After a few epochs (5 here), we **unfreeze the backbone** for fine-tuning, letting the whole network adjust and improve performance.\n",
    "\n",
    "3. **Saving the Best Model**\n",
    "    - We save the model whenever validation loss improves so we don’t lose the best version.  \n",
    "\n",
    "This is an example AI used:\n",
    "- First, they copy the teacher’s work (frozen backbone) to get the basics.  \n",
    "- Later, they start experimenting on their own (unfrozen backbone) to get even better.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32e90ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train Loss: 13.3787 | Val Loss: 12.8048 | Age Train MAE: 13.4364 | Age Val MAE: 12.8543\n",
      "Best model saved\n",
      "Epoch 02 | Train Loss: 13.1423 | Val Loss: 12.5692 | Age Train MAE: 13.1980 | Age Val MAE: 12.6157\n",
      "Best model saved\n",
      "Epoch 03 | Train Loss: 12.8745 | Val Loss: 12.4837 | Age Train MAE: 12.9327 | Age Val MAE: 12.5314\n",
      "Best model saved\n",
      "Epoch 04 | Train Loss: 12.6879 | Val Loss: 12.4356 | Age Train MAE: 12.7552 | Age Val MAE: 12.4790\n",
      "Best model saved\n",
      "Epoch 05 | Train Loss: 12.6001 | Val Loss: 12.2701 | Age Train MAE: 12.6652 | Age Val MAE: 12.3193\n",
      "Best model saved\n",
      "Backbone unfrozen for fine-tuning\n",
      "Epoch 06 | Train Loss: 8.2748 | Val Loss: 7.1728 | Age Train MAE: 8.3650 | Age Val MAE: 7.3020\n",
      "Best model saved\n",
      "Epoch 07 | Train Loss: 5.6151 | Val Loss: 5.6156 | Age Train MAE: 5.7871 | Age Val MAE: 5.7659\n",
      "Best model saved\n",
      "Epoch 08 | Train Loss: 4.4562 | Val Loss: 5.7351 | Age Train MAE: 4.6661 | Age Val MAE: 5.8910\n",
      "Epoch 09 | Train Loss: 3.8225 | Val Loss: 6.0925 | Age Train MAE: 4.0620 | Age Val MAE: 6.2464\n",
      "Epoch 10 | Train Loss: 3.5324 | Val Loss: 5.2199 | Age Train MAE: 3.7994 | Age Val MAE: 5.3904\n",
      "Best model saved\n",
      "Epoch 11 | Train Loss: 3.4073 | Val Loss: 5.3041 | Age Train MAE: 3.6960 | Age Val MAE: 5.4853\n",
      "Epoch 12 | Train Loss: 3.1338 | Val Loss: 5.1136 | Age Train MAE: 3.4437 | Age Val MAE: 5.2852\n",
      "Best model saved\n",
      "Epoch 13 | Train Loss: 2.8160 | Val Loss: 4.7867 | Age Train MAE: 3.1375 | Age Val MAE: 4.9541\n",
      "Best model saved\n",
      "Epoch 14 | Train Loss: 2.7702 | Val Loss: 4.8142 | Age Train MAE: 3.1117 | Age Val MAE: 4.9643\n",
      "Epoch 15 | Train Loss: 2.6246 | Val Loss: 4.8360 | Age Train MAE: 2.9862 | Age Val MAE: 5.0050\n",
      "Epoch 16 | Train Loss: 2.3911 | Val Loss: 4.7958 | Age Train MAE: 2.7554 | Age Val MAE: 4.9366\n",
      "Epoch 17 | Train Loss: 2.3392 | Val Loss: 4.5424 | Age Train MAE: 2.7096 | Age Val MAE: 4.6762\n",
      "Best model saved\n",
      "Epoch 18 | Train Loss: 2.1518 | Val Loss: 4.6602 | Age Train MAE: 2.5316 | Age Val MAE: 4.7980\n",
      "Epoch 19 | Train Loss: 2.0469 | Val Loss: 4.5522 | Age Train MAE: 2.4377 | Age Val MAE: 4.6884\n",
      "Epoch 20 | Train Loss: 2.0286 | Val Loss: 4.4816 | Age Train MAE: 2.4243 | Age Val MAE: 4.6167\n",
      "Best model saved\n",
      "Epoch 21 | Train Loss: 1.9118 | Val Loss: 4.5798 | Age Train MAE: 2.3088 | Age Val MAE: 4.7144\n",
      "Epoch 22 | Train Loss: 1.6710 | Val Loss: 4.4200 | Age Train MAE: 2.0638 | Age Val MAE: 4.5156\n",
      "Best model saved\n",
      "Epoch 23 | Train Loss: 1.7294 | Val Loss: 4.5486 | Age Train MAE: 2.1286 | Age Val MAE: 4.6456\n",
      "Epoch 24 | Train Loss: 1.6025 | Val Loss: 4.4287 | Age Train MAE: 2.0023 | Age Val MAE: 4.5149\n",
      "Epoch 25 | Train Loss: 1.4908 | Val Loss: 4.4389 | Age Train MAE: 1.8845 | Age Val MAE: 4.5214\n",
      "Epoch 26 | Train Loss: 1.4568 | Val Loss: 4.4411 | Age Train MAE: 1.8578 | Age Val MAE: 4.5435\n",
      "Epoch 27 | Train Loss: 1.4946 | Val Loss: 4.5503 | Age Train MAE: 1.8977 | Age Val MAE: 4.6322\n",
      "Early stopping activated\n"
     ]
    }
   ],
   "source": [
    "# ------ Training Loop (Fixed Shapes & Fun Notes) ------\n",
    "\n",
    "\n",
    "mae_fn = L1Loss()\n",
    "best_val_loss = float(\"inf\")  # Just making sure the first epoch be saved .\n",
    "patience, no_improve = 5, 0\n",
    "\n",
    "for epoch in range(1, 31):\n",
    "    # ---- Train ----\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    total_train_mae = 0\n",
    "    for imgs, genders, ages in train_loader:\n",
    "        imgs, genders, ages = imgs.to(device), genders.to(device), ages.to(device)\n",
    "        genders = genders.view(-1, 1)\n",
    "        ages = ages.view(-1, 1)\n",
    "\n",
    "        opt.zero_grad()\n",
    "\n",
    "        g_out, a_out = model(imgs)\n",
    "\n",
    "        loss = loss_gender(g_out, genders) + loss_age(a_out, ages)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        total_train_loss += loss.item() * imgs.size(0)\n",
    "        total_train_mae += mae_fn(a_out, ages).item() * imgs.size(0)\n",
    "\n",
    "    # ---- Validate ----\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    total_val_mae = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, genders, ages in val_loader:\n",
    "            imgs, genders, ages = imgs.to(device), genders.to(device), ages.to(device)\n",
    "            genders = genders.view(-1, 1)\n",
    "            ages = ages.view(-1, 1)\n",
    "\n",
    "            g_out, a_out = model(imgs)\n",
    "            loss = loss_gender(g_out, genders) + loss_age(a_out, ages)\n",
    "\n",
    "            total_val_loss += loss.item() * imgs.size(0)\n",
    "            total_val_mae += mae_fn(a_out, ages).item() * imgs.size(0)\n",
    "\n",
    "    # Compute averages\n",
    "    train_loss = total_train_loss / len(train_loader.dataset)\n",
    "    val_loss = total_val_loss / len(val_loader.dataset)\n",
    "    train_mae = total_train_mae / len(train_loader.dataset)\n",
    "    val_mae = total_val_mae / len(val_loader.dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | \"\n",
    "          f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
    "          f\"Age Train MAE: {train_mae:.4f} | Age Val MAE: {val_mae:.4f}\")\n",
    "\n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"best_age_gender.pth\")\n",
    "        print(\"Best model saved\")\n",
    "        no_improve = 0\n",
    "    else:\n",
    "        no_improve += 1\n",
    "\n",
    "    # Unfreeze backbone after a few epochs\n",
    "    if epoch == 5:\n",
    "        for p in model.backbone.parameters():\n",
    "            p.requires_grad = True\n",
    "        opt = optim.Adam(model.parameters(), lr=1e-4)\n",
    "        print(\"Backbone unfrozen for fine-tuning\")\n",
    "\n",
    "    # Early stopping\n",
    "    if no_improve >= patience:\n",
    "        print(\"Early stopping activated\")\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bbc405",
   "metadata": {},
   "source": [
    "### Training Highlights\n",
    "\n",
    "- **Total epochs:** 27  \n",
    "- **Best validation loss:** 4.42  \n",
    "- **Early stopping activated** at epoch 27  \n",
    "\n",
    "### Key Observation: Epoch 5 → Epoch 6\n",
    "\n",
    "- **Epoch 5:** Backbone frozen → small, gradual improvements  \n",
    "- **Epoch 6:** Backbone unfrozen → major performance jump  \n",
    "  - Train Loss dropped sharply  \n",
    "  - Val Loss dropped sharply  \n",
    "  - Age MAE improved by ~4 years  \n",
    "- This marks the turning point where fine-tuning the pretrained backbone dramatically improves the model.\n",
    "\n",
    "\n",
    "### Time For Testing The Model\n",
    "Now that we trained our Model with a good MAE for age (about 4.5 years) , we are going to test it on test_image.jpg , it will be saved in test_image_output.jpg\n",
    "\n",
    "#### X Important X\n",
    "In tests, I used **haar** algorithm for detecting face before estimating . Since haar is not the best option for face detection , it might not perform very well . I suggest using other methods for detecting face such as **Dlib Face Detector** or **YOLO**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0b72dbd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"best_age_gender.pth\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "frame = cv2.imread(\"test_image.jpg\")\n",
    "\n",
    "gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "for (x, y, w, h) in faces:\n",
    "    face_img = frame[y:y+h, x:x+w]\n",
    "    img_pil = Image.fromarray(cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB))\n",
    "    img_t = val_tf(img_pil).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        g_logit, a_pred = model(img_t)\n",
    "        gender_prob = torch.sigmoid(g_logit).item()\n",
    "        gender_label = \"Female\" if gender_prob > 0.5 else \"Male\"\n",
    "        age_val = float(a_pred.item())\n",
    "        age_val = max(0.0, min(116.0, age_val))  # clamp\n",
    "\n",
    "    # Scaling factors relative to face size\n",
    "    thickness = max(1, w // 100)  # rectangle & text thickness\n",
    "    font_scale = w / 200          # font size\n",
    "\n",
    "    # Draw rectangle\n",
    "    cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), thickness)\n",
    "\n",
    "    # Prepare text\n",
    "    text = f\"{gender_label} ({(gender_prob*100) if gender_prob > 0.5 else (100 - gender_prob*100):.0f}%), Age: {age_val:.0f}\"\n",
    "    text_y = max(0, y - 10)  # keep text inside image\n",
    "    cv2.putText(frame, text, (x, text_y),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, font_scale, (0, 255, 0), thickness)\n",
    "\n",
    "cv2.imwrite(\"test_image_output.jpg\", frame)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
